{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from architectures import *\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms, datasets\n",
    "from utils import *\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 37\n",
    "ROOT_PATH = os.getcwd()\n",
    "os.makedirs(DATASET_PATH := os.path.join(ROOT_PATH, \"dataset\"), exist_ok=True)\n",
    "os.makedirs(MODELS_PATH := os.path.join(ROOT_PATH, \"models\"), exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_RATIO, DEV_RATIO, TEST_RATIO = 0.8, 0.1, 0.1\n",
    "LBP_ENGINEERING = False\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 50\n",
    "PATIENCE = 10\n",
    "\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 224, 224\n",
    "\n",
    "seed_functions(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = transforms.Compose([transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)), transforms.ToTensor()])\n",
    "dataset = datasets.ImageFolder(DATASET_PATH, transform=T)\n",
    "\n",
    "def split_data(dataset):\n",
    "\tlabels = [tp[1] for tp in dataset]\n",
    "\tprint(\"Class balance:\", dict(Counter(labels)))\n",
    "\ttrain_set, temp_set = train_test_split(dataset, test_size=(1 - TRAIN_RATIO), random_state=SEED, stratify=labels)\n",
    "\ttemp_labels = [tp[1] for tp in temp_set]\n",
    "\ttest_set, dev_set = train_test_split(temp_set, test_size=(TEST_RATIO / (DEV_RATIO + TEST_RATIO)), random_state=SEED, stratify=temp_labels)\n",
    "\tif LBP_ENGINEERING:\n",
    "\t\treturn LBPDataset(train_set), LBPDataset(dev_set), LBPDataset(test_set)\n",
    "\treturn train_set, dev_set, test_set\n",
    "\n",
    "train_set, dev_set, test_set = split_data(dataset)\n",
    "train_generator = torch.Generator().manual_seed(SEED)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, generator=train_generator)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = R50ViTB16().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def freeze_model(model, frozen_dict):\n",
    "\tif not getattr(model, \"freezable\", False):\n",
    "\t\treturn\n",
    "\tcfg = frozen_dict.get(model.id)\n",
    "\tif cfg is None:\n",
    "\t\treturn\n",
    "\tprefix = cfg[\"freeze_until\"]\n",
    "\tprint(f\"[Freezing] model.id={model.id}, freeze_until={prefix}\")\n",
    "\tfor name, param in model.named_parameters():\n",
    "\t\tprint(f\"  - {name}\")\n",
    "\t\tparam.requires_grad = False\n",
    "\t\tif name.startswith(prefix):\n",
    "\t\t\tbreak\n",
    "\n",
    "def train_model(model, train_loader, dev_loader, criterion, optimizer):\n",
    "\t# Create model folder\n",
    "\tmodel_folder = os.path.join(MODELS_PATH, f\"{model.id}\")\n",
    "\tos.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "\tbest_dev_loss = float(\"inf\")\n",
    "\tbest_model_path = os.path.join(model_folder, \"best.pth\")\n",
    "\tpatience_counter = 0\n",
    "\ttrain_losses = []\n",
    "\tdev_losses = []\n",
    "\n",
    "\tfor epoch in range(NUM_EPOCHS):\n",
    "\t\t# Train model on train set\n",
    "\t\tmodel.train()\n",
    "\t\ttotal_loss = 0\n",
    "\t\tfor images, labels in train_loader:\n",
    "\t\t\timages, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\toutputs = model(images)\n",
    "\t\t\tloss = criterion(outputs, labels)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\t\ttrain_loss = total_loss / len(train_loader)\n",
    "\t\ttrain_losses.append(train_loss)\n",
    "\n",
    "\t\t# Evaluate model on dev set\n",
    "\t\tmodel.eval()\n",
    "\t\tdev_loss = 0\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor images, labels in dev_loader:\n",
    "\t\t\t\timages, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\t\t\t\toutputs = model(images)\n",
    "\t\t\t\tdev_loss += criterion(outputs, labels).item()\n",
    "\t\tdev_loss /= len(dev_loader)\n",
    "\t\tdev_losses.append(dev_loss)\n",
    "\n",
    "\t\tprint(f\"Epoch: {epoch+1} of {NUM_EPOCHS} - Training loss: {train_loss:.4f} - Validation loss: {dev_loss:.4f}\")\n",
    "\n",
    "\t\t# Save model epoch checkpoint\n",
    "\t\ttorch.save(model.state_dict(), os.path.join(model_folder, f\"epoch={epoch+1}-dev_loss={dev_loss:.4f}.pth\"))\n",
    "\t\tif dev_loss < best_dev_loss:\n",
    "\t\t\tbest_dev_loss = dev_loss\n",
    "\t\t\ttorch.save(model.state_dict(), best_model_path)  # Save best model\n",
    "\t\t\tpatience_counter = 0  # Reset patience counter\n",
    "\t\telse:\n",
    "\t\t\tpatience_counter += 1\n",
    "\t\t\t# Stop early if needed\n",
    "\t\t\tif patience_counter >= PATIENCE:\n",
    "\t\t\t\tprint(\"Stopping early\")\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t# Save losses\n",
    "\tloss_data = {\"train_losses\": train_losses, \"dev_losses\": dev_losses}\n",
    "\twith open(os.path.join(model_folder, \"losses.json\"), \"w\") as f:\n",
    "\t\tjson.dump(loss_data, f)\n",
    "\n",
    "# freeze_model(model, {\"RN50\": {\"freeze_until\": \"layer3\"}})\n",
    "train_model(model, train_loader, dev_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_loss(model):\n",
    "\t# Load losses\n",
    "\tmodel_folder = os.path.join(MODELS_PATH, f\"{model.id}\")\n",
    "\twith open(os.path.join(model_folder, \"losses.json\"), \"r\") as f:\n",
    "\t\tloss_data = json.load(f)\n",
    "\ttrain_losses = loss_data[\"train_losses\"]\n",
    "\tdev_losses = loss_data[\"dev_losses\"]\n",
    "\n",
    "\tplt.figure(figsize=(8, 5))\n",
    "\tepochs = range(1, len(train_losses) + 1)\n",
    "\tplt.plot(epochs, train_losses, \"b-\", label=\"Training loss\")\n",
    "\tplt.plot(epochs, dev_losses, \"r-\", label=\"Validation loss\")\n",
    "\n",
    "\t# Annotate the best loss\n",
    "\tbest_epoch = np.argmin(dev_losses) + 1\n",
    "\tbest_loss = min(dev_losses)\n",
    "\tplt.plot(best_epoch, best_loss, \"ro\", markersize=8, label=f\"Best loss: {best_loss:.4f}\")\n",
    "\n",
    "\tplt.xlabel(\"Epoch\")\n",
    "\tplt.ylabel(\"Loss\")\n",
    "\tplt.legend()\n",
    "\tplt.gca().xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "\tmodel.eval()\n",
    "\tY_hat, Y = [], []\n",
    "\twith torch.no_grad():\n",
    "\t\tfor x, y in dataloader:\n",
    "\t\t\tlogits = model(x.to(DEVICE))\n",
    "\t\t\tY_hat.append(torch.argmax(logits, dim=1).cpu())\n",
    "\t\t\tY.append(y.view(-1))\n",
    "\tY_hat, Y = torch.cat(Y_hat), torch.cat(Y)\n",
    "\ttn, fp, fn, tp = confusion_matrix(Y, Y_hat).ravel()\n",
    "\tbinary_f1 = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) != 0 else 0\n",
    "\tmacro_f1 = f1_score(Y, Y_hat, average=\"macro\")\n",
    "\treturn {\n",
    "\t\t\"tp\": int(tp),\n",
    "\t\t\"tn\": int(tn),\n",
    "\t\t\"fp\": int(fp),\n",
    "\t\t\"fn\": int(fn),\n",
    "\t\t\"macro_f1\": float(macro_f1)\n",
    "\t}\n",
    "\n",
    "m = model.__class__().to(DEVICE)\n",
    "visualise_loss(m)\n",
    "\n",
    "m.load_state_dict(torch.load(os.path.join(MODELS_PATH, f\"{m.id}\", \"best.pth\")))\n",
    "print(evaluate_model(m, test_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
